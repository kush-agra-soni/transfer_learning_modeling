# ğŸŒ¸ Transfer Learning on Oxford Flowers 102

### *Deep Learning Meets Gardening*

Welcome to **Transfer Learning on Oxford Flowers 102**, where neural networks learn to tell a *Sunflower* from a *Snowdrop*â€”even when you canâ€™t. This project uses three popular pre-trained CNN architecturesâ€”**ResNet50**, **VGG16**, and **MobileNetV2**â€”to tackle the beautifully challenging **Oxford Flowers 102** dataset.

Think of it as teaching world-class ImageNet models to become refined botanists. Some do greatâ€¦ others forget their roots.

---

## ğŸŒ¼ Project Overview

This project demonstrates how **transfer learning** can be used to train powerful image classifiers with limited data. Using TensorFlow and TensorFlow Datasets, we:

* Load and preprocess the *Oxford Flowers 102* dataset
* Plug images into three different pre-trained models
* Fine-tune each model for flower classification
* Evaluate performance and compare architectures
* Plot training curves and draw meaningful insights

Whether youâ€™re into deep learning, flowers, or both, there's something blooming here for you.

---

## ğŸ“‚ Dataset: Oxford Flowers 102

The **Oxford Flowers 102** dataset is a fine-grained image classification dataset featuring:

* **102 flower classes**
* Highly variable poses, lighting, scales, and backgrounds
* Pre-defined splits:

  * **Train:** 1020 images
  * **Validation:** 1020 images
  * **Test:** 6149 images

Every flower is prettier than the last, but the models donâ€™t get distracted (usually).

---

## ğŸ§ª Models Used

### ğŸ”µ ResNet50

A very deep model with residual connections. Brilliant on CIFAR and ImageNetâ€¦ surprisingly confused by flowers here.

### ğŸŸ£ VGG16

A classic architecture with stacked convolutional layers. Turns out VGG is the true flower enthusiastâ€”best accuracy in this project.

### ğŸŸ¢ MobileNetV2

Lightweight and mobile-friendly. Fast, efficient, and unexpectedly baffled by petals.

---

## ğŸ”§ Workflow Summary

### 1ï¸âƒ£ Data Loading & Preprocessing

* Using `tfds.load()`
* Resize images â†’ **224 Ã— 224**
* Apply model-specific preprocessing
* One-hot encode labels
* Batch + prefetch for GPU efficiency

### 2ï¸âƒ£ Model Preparation

Each model is loaded **without the top layer**, then extended using:

* `GlobalAveragePooling2D`
* Dense layer(s)
* Dropout
* 102-class softmax output

Base layers are initially **frozen** to preserve ImageNet wisdom.

### 3ï¸âƒ£ Fine-Tuning

We partially unfreeze the top layers of each model:

* ResNet50 â†’ last 30 layers
* VGG16 â†’ last 5 layers
* MobileNetV2 â†’ last 40 layers

Then train for **3 epochs** (demo purpose, adjust as needed).

### 4ï¸âƒ£ Evaluation

---

## ğŸ“Š Results

| Model           | Accuracy  | Notes                                   |
| --------------- | --------- | --------------------------------------- |
| **VGG16**       | â­ **48%** | Clear winnerâ€”VGG really loves flowers   |
| **ResNet50**    | 12%       | Shockingly lowâ€”still lost in the garden |
| **MobileNetV2** | 10%       | Lightweight, but not a botanist         |

**Key takeaway:** sometimes the â€œsimplestâ€ model is the best fit for fine-grained classification.

---

## ğŸ’¡ Insights & Discussion

* **Transfer learning works**, even with small datasetsâ€”but model choice matters a lot.
* **VGG16 consistently outperforms** deeper or more efficient models here, likely due to its feature extraction style.
* Fine-grained classification can be trickyâ€”flower species vary subtly.
* Preprocessing and careful unfreezing of layers can significantly impact results.

Optional improvements:

* Data augmentation
* More fine-tuning
* Learning rate schedules
* Confusion matrix analysis
* Trying EfficientNet or ViT models

---

## ğŸ“ Repository Structure (suggested)

```
ğŸ“¦ flower-transfer-learning
 â”œâ”€â”€ data/                 # tfds-managed dataset
 â”œâ”€â”€ models/               # saved models
 â”œâ”€â”€ plots/                # accuracy & loss graphs
 â”œâ”€â”€ train.py              # training loop
 â”œâ”€â”€ evaluate.py           # evaluation script
 â”œâ”€â”€ README.md             # you're reading it!
 â””â”€â”€ requirements.txt
```

---

## ğŸš€ Getting Started

### Install dependencies

```bash
pip install tensorflow tensorflow-datasets matplotlib
```

### Run training

```bash
python train.py
```

### Evaluate models

```bash
python evaluate.py
```

---

## ğŸŒº Final Thoughts

This project shows how transfer learning transforms powerful general-purpose CNNs into flower specialists. While some architectures flourished, others... wilted. But every experiment helps us grow (pun proudly intended).

If you like deep learning, botany, or models that struggle to distinguish roses from rhododendrons, youâ€™ll enjoy exploring this codebase.

Happy trainingâ€”and may your validation accuracy blossom! ğŸŒ·
